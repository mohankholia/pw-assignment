{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d993f40d-1a4a-44f3-bd22-f7e15cf629f4",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator Regression, is a linear regression technique used for both prediction and feature selection. It differs from other regression techniques, such as ordinary least squares (OLS) regression and Ridge Regression, in the way it adds a regularization term to the linear regression equation. This regularization term penalizes the absolute values of the coefficients, encouraging some coefficients to be exactly zero, effectively performing feature selection.\n",
    "\n",
    "# Q2. The main advantage of using Lasso Regression in feature selection is that it automatically selects a subset of the most relevant features while setting the coefficients of less important features to zero. This feature selection property helps in building simpler and more interpretable models, reducing the risk of overfitting, and improving the model's generalization performance.\n",
    "\n",
    "# Q3. \n",
    "The coefficients of a Lasso Regression model can be interpreted as follows:\n",
    "   - Non-zero coefficients: These coefficients indicate the features that have a significant impact on the target variable. The larger the absolute value of a non-zero coefficient, the stronger the feature's impact.\n",
    "   - Zero coefficients: Coefficients set to zero indicate that the corresponding feature has been effectively excluded from the model, implying that it does not contribute to the prediction of the target variable.\n",
    "\n",
    "# Q4.\n",
    "Lasso Regression has one tuning parameter: the regularization strength parameter, often denoted as lambda (λ). Adjusting λ affects the model's performance as follows:\n",
    "   - Smaller λ values: These lead to weaker regularization, allowing the model to fit the training data more closely. This can result in overfitting if λ is too small.\n",
    "   - Larger λ values: These increase the strength of regularization, encouraging sparsity in the coefficients and feature selection. Using too large a λ may lead to underfitting if important features are penalized too heavily.\n",
    "\n",
    "# Q5.\n",
    "Lasso Regression is primarily designed for linear regression problems. However, it can be extended to handle non-linear regression problems by applying non-linear transformations to the input features before performing Lasso Regression. For example, you can use polynomial features or other basis functions to capture non-linear relationships in the data. This effectively transforms the problem into a linear regression problem in the transformed feature space.\n",
    "\n",
    "# Q6.\n",
    "The main difference between Ridge Regression and Lasso Regression lies in the type of regularization applied:\n",
    "   - Ridge Regression uses L2 regularization, which penalizes the square of the coefficients. It encourages small but non-zero coefficients for all features.\n",
    "   - Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients. It encourages some coefficients to be exactly zero, leading to feature selection.\n",
    "\n",
    "# Q7.\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent. Multicollinearity occurs when two or more input features are highly correlated. Lasso's L1 regularization tends to select one of the correlated features while setting the coefficients of the others to zero. By doing so, it effectively chooses which features to keep and which to discard, helping to address multicollinearity.\n",
    "\n",
    "# Q8.\n",
    "Choosing the optimal value of the regularization parameter (λ) in Lasso Regression can be done through techniques such as cross-validation. You can train the Lasso model with different values of λ and evaluate its performance on a validation set or through cross-validation. The value of λ that results in the best model performance (e.g., lowest mean squared error for regression problems) on the validation data is considered the optimal choice. Alternatively, you can use techniques like the Lasso path, which visualizes the path of coefficients as λ varies, helping you identify the appropriate λ based on the desired level of sparsity and predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04262e9-6c67-4741-beb6-e4b053f721c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
